---
layout: post
comments: true
mathjax: true
priority: 440000
title: “Common information retrieval evaluation metrics”
tags: [information retrieval]
excerpt: information retrieval
img: ir.png
date: 2019-07-09 12:00:00
---

## Rank-Based Measures:

1. Binary relevance
* Precision@K (P@K)
* Mean Average Precision (MAP)
* Mean Reciprocal Rank (MRR)

2. Multiple levels of relevance
* Normalized Discounted Cumulative Gain (NDCG)

## Precision@K

1. Set a rank threshold K
2. Compute % relevant in top K
3. Ignores documents ranked lower than K
4. Example: 
    * Prec@3 of 2/3
    * Prec@4 of 2/4
    * Prec@5 of 3/5

<div class="imgcap">
<img src="https://user-images.githubusercontent.com/22668421/60916015-5dbe7f80-a25b-11e9-9022-e1ebfd592b4e.png" style="border:none;width:50%">
</div>


## Mean Average Precision
1. Consider rank position of each relevant doc $$K_{1}$$, $$K_{2}$$,..., $$K_{R}$$.
2. Compute Precision@K for each $$K_{1}$$, $$K_{2}$$,..., $$K_{R}$$
3. Average precision = average of P@K
4. Example:
$$
\frac{1}{3} \cdot\left(\frac{1}{1}+\frac{2}{3}+\frac{3}{5}\right) \approx 0.76
$$
<div class="imgcap">
<img src="https://user-images.githubusercontent.com/22668421/60916015-5dbe7f80-a25b-11e9-9022-e1ebfd592b4e.png" style="border:none;width:50%">
</div>

5. MAP is Average Precision across multiple queries/rankings
6. MAP is macro-averaging: each query counts equally

## When There’s only 1 Relevant Document
1. Scenarios: 
    * known-item search
    * navigational queries
    * looking for a fact
2. Search Length = Rank of the answer
    * measures a user’s effort


## Mean Reciprocal Rank
1. Consider rank position, K, of first relevant doc
2. Reciprocal Rank score $$=\frac{1}{K}$$
3. MRR is the mean RR across multiple queries 


   



  



