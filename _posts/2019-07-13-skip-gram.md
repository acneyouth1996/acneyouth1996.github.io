---
layout: post
comments: true
mathjax: true
priority: 440000
title: “Word2Vec and skip gram model”
tags: [NLP]
excerpt: NLP
img: nlp.png
date: 2019-07-13 12:00:00
---

This post combines part of Jay Alammar's "[The Illustrated word2vec](http://jalammar.github.io/illustrated-word2vec/)" blog, this blog is very well written, so I shamelessly stole it's word2vec explanation part in here.


## cosine similarity vs euclidean distance 
Why cosine similarity is more often used in evaluating similarity of word embedding, any reason behind it? here we will break it down to see why.

Euclidean distance function can be defined as follows:

$$
\sqrt{\sum_{i=1}^{n}\left(x_{i}-y_{i}\right)^{2}}
$$

Cosine similarity function can be defined as follows:

$$
\frac{x \bullet y}{\sqrt{x \bullet x} \sqrt{y \bullet y}}
$$

where $$x$$ $$y$$ are two vectors.

It is easy to see that Cosine is essentially the same as Euclidean on normalized data. The normalization takes away one degree of freedom. Thus, cosine on a 1000 dimensional space is about as "cursed" as Euclidean on a 999 dimensional space.

What is usually different is the data where you would use one vs. the other. Euclidean is commonly used on dense, continuous variables. There every dimension matters, and a 20 dimensional space can be challenging. Cosine is mostly used on very sparse, discrete domains such as text. Here, most dimensions are 0 and do not matter at all. A 100.000 dimensional vector space may have just some 50 nonzero dimensions for a distance computation; and of these many will have a low weight (stopwords). So it is the typical use case of cosine that is not cursed, even though it theoretically is a very high dimensional space.

## Word Embeddings

Let's hop into word embeddings. We can use a vector to represent a word, and this vector is called word embeddings. and The famous examples that show an incredible property of embeddings is the concept of analogies. We can add and subtract word embeddings and arrive at interesting results. The most famous example is the formula: “king” - “man” + “woman”:

<div class="imgcap">
<img src="https://user-images.githubusercontent.com/22668421/61177738-f29de180-a5a9-11e9-980a-11ad7f1032a1.png" style="border:none;width:100%">
</div>


## Language Modeling

Next-word prediction is a task that can be addressed by a language model. A language model can take a list of words, and attempt to predict the word that follows them.

<div class="imgcap">
<img src="https://user-images.githubusercontent.com/22668421/61177905-27139c80-a5ae-11e9-93c0-5e53b0187060.png" style="border:none;width:100%">
</div>

We can think of the model as looking like this black box:

<div class="imgcap">
<img src="https://user-images.githubusercontent.com/22668421/61177910-5b875880-a5ae-11e9-85a0-08429cb74596.png" style="border:none;width:100%">
</div>

But in practice, the model doesn’t output only one word. It actually outputs a probability score for all the words it knows (the model’s “vocabulary”, which can range from a few thousand to over a million words). The keyboard application then has to find the words with the highest scores, and present those to the user.
<div class="imgcap">
<img src="https://user-images.githubusercontent.com/22668421/61177912-8376bc00-a5ae-11e9-9832-5f271a250ae5.png" style="border:none;width:100%">
</div>

Words get their embeddings by us looking at which other words they tend to appear next to. The mechanics of that is that

1. We get a lot of text data (say, all Wikipedia articles, for example). then
2. We have a window (say, of three words) that we slide against all of that text.
3. The sliding window generates training samples for our model

As this window slides against the text, we (virtually) generate a dataset that we use to train a model. To look exactly at how that’s done, let’s see how the sliding window processes this phrase:

Thou shalt not make a machine in the likeness of a human mind” ~Dune

When we start, the window is on the first three words of the sentence:

<div class="imgcap">
<img src="https://user-images.githubusercontent.com/22668421/61177938-8de58580-a5af-11e9-8215-d7a6e92196ea.png" style="border:none;width:100%">
</div>

We take the first two words to be features, and the third word to be a label，We then slide our window to the next position and create a second sample




## Skipgram

Instead of only looking two words before the target word, we can also look at two words after it.
<div class="imgcap">
<img src="https://user-images.githubusercontent.com/22668421/61177987-7ce94400-a5b0-11e9-826d-fef530591b3b.png" style="border:none;width:100%">
</div>

If we do this, the dataset we’re virtually building and training the model against would look like this:
<div class="imgcap">
<img src="https://user-images.githubusercontent.com/22668421/61177995-9a1e1280-a5b0-11e9-9ee0-996a1660d8f1.png" style="border:none;width:100%">
</div>

This is called a Continuous Bag of Words architecture and is described in one of the word2vec papers.
Another architecture that also tended to show great results does things a little differently.
Instead of guessing a word based on its context (the words before and after it), this other architecture tries to guess neighboring words using the current word. We can think of the window it slides against the training text as looking like this:
<div class="imgcap">
<img src="https://user-images.githubusercontent.com/22668421/61178005-dbaebd80-a5b0-11e9-9834-9fb2284b3703.png" style="border:none;width:100%">
</div>

The pink boxes are in different shades because this sliding window actually creates four separate samples in our training dataset:
<div class="imgcap">
<img src="https://user-images.githubusercontent.com/22668421/61178010-0a2c9880-a5b1-11e9-88fe-eb98efa474cf.png" style="border:none;width:100%">
</div>

This method is called the skipgram architecture. We can visualize the sliding window as doing the following
<div class="imgcap">
<img src="https://user-images.githubusercontent.com/22668421/61178013-2597a380-a5b1-11e9-9608-bd4f2d2a3a7f.png" style="border:none;width:100%">
</div>

This would add these four samples to our training dataset.

Now that we have our skipgram training dataset that we extracted from existing running text, let’s glance at how we use it to train a basic neural language model that predicts the neighboring word.

<div class="imgcap">
<img src="https://user-images.githubusercontent.com/22668421/61178019-5546ab80-a5b1-11e9-833f-78c447c4ef58.png" style="border:none;width:100%">
</div>











