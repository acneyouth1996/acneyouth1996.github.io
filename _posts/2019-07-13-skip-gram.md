---
layout: post
comments: true
mathjax: true
priority: 440000
title: “Word2Vec and skip gram model”
tags: [NLP]
excerpt: NLP
img: nlp.png
date: 2019-07-13 12:00:00
---

This post combines part of Jay Alammar's "[The Illustrated word2vec](http://jalammar.github.io/illustrated-word2vec/)" blog, this blog is very well written, so I shamelessly stole it's word2vec explanation part in here.


## cosine similarity vs euclidean distance 
Why cosine similarity is more often used in evaluating similarity of word embedding, any reason behind it? here we will break it down to see why.

Euclidean distance function can be defined as follows:

$$
\sqrt{\sum_{i=1}^{n}\left(x_{i}-y_{i}\right)^{2}}
$$

Cosine similarity function can be defined as follows:

$$
\frac{x \bullet y}{\sqrt{x \bullet x} \sqrt{y \bullet y}}
$$

where $$x$$ $$y$$ are two vectors.

It is easy to see that Cosine is essentially the same as Euclidean on normalized data. The normalization takes away one degree of freedom. Thus, cosine on a 1000 dimensional space is about as "cursed" as Euclidean on a 999 dimensional space.

What is usually different is the data where you would use one vs. the other. Euclidean is commonly used on dense, continuous variables. There every dimension matters, and a 20 dimensional space can be challenging. Cosine is mostly used on very sparse, discrete domains such as text. Here, most dimensions are 0 and do not matter at all. A 100.000 dimensional vector space may have just some 50 nonzero dimensions for a distance computation; and of these many will have a low weight (stopwords). So it is the typical use case of cosine that is not cursed, even though it theoretically is a very high dimensional space.

## Word Embeddings

Let's hop into word embeddings. We can use a vector to represent a word, and this vector is called word embeddings. and The famous examples that show an incredible property of embeddings is the concept of analogies. We can add and subtract word embeddings and arrive at interesting results. The most famous example is the formula: “king” - “man” + “woman”:

<div class="imgcap">
<img src="https://user-images.githubusercontent.com/22668421/61177738-f29de180-a5a9-11e9-980a-11ad7f1032a1.png" style="border:none;width:100%">
</div>




