---
layout: post
comments: true
mathjax: true
priority: 440000
title: “Word2Vec and skip gram model”
tags: [NLP]
excerpt: NLP
img: es.jpeg
date: 2019-07-13 12:00:00
---

This post combines part of Jay Alammar's "[The Illustrated word2vec](http://jalammar.github.io/illustrated-word2vec/)" blog, this blog is very well written, so I shamelessly stole it's word2vec explanation part in here.


## cosine similarity vs euclidean distance 
Why cosine similarity is more often used in evaluating similarity of word embedding, any reason behind it? here we will break it down to see why.

Euclidean distance function can be defined as follows:

$$
\sqrt{\sum_{i=1}^{n}\left(x_{i}-y_{i}\right)^{2}}
$$

Cosine similarity function can be defined as follows:

$$
\frac{x \bullet y}{\sqrt{x \bullet x} \sqrt{y \bullet y}}
$$

where $$x$$ $$y$$ are two vectors.

It is easy to see that Cosine is essentially the same as Euclidean on normalized data. The normalization takes away one degree of freedom. Thus, cosine on a 1000 dimensional space is about as "cursed" as Euclidean on a 999 dimensional space.

What is usually different is the data where you would use one vs. the other. Euclidean is commonly used on dense, continuous variables. There every dimension matters, and a 20 dimensional space can be challenging. Cosine is mostly used on very sparse, discrete domains such as text. Here, most dimensions are 0 and do not matter at all. A 100.000 dimensional vector space may have just some 50 nonzero dimensions for a distance computation; and of these many will have a low weight (stopwords). So it is the typical use case of cosine that is not cursed, even though it theoretically is a very high dimensional space.

## Word Embeddings

Let's hop into word embeddings. We can use a vector to represent a word, and this vector is called word embeddings. and The famous examples that show an incredible property of embeddings is the concept of analogies. We can add and subtract word embeddings and arrive at interesting results. The most famous example is the formula: “king” - “man” + “woman”:

<div class="imgcap">
<img src="https://user-images.githubusercontent.com/22668421/61177738-f29de180-a5a9-11e9-980a-11ad7f1032a1.png" style="border:none;width:100%">
</div>


## Language Modeling

Next-word prediction is a task that can be addressed by a language model. A language model can take a list of words, and attempt to predict the word that follows them.

<div class="imgcap">
<img src="https://user-images.githubusercontent.com/22668421/61177905-27139c80-a5ae-11e9-93c0-5e53b0187060.png" style="border:none;width:100%">
</div>

We can think of the model as looking like this black box:

<div class="imgcap">
<img src="https://user-images.githubusercontent.com/22668421/61177910-5b875880-a5ae-11e9-85a0-08429cb74596.png" style="border:none;width:100%">
</div>

But in practice, the model doesn’t output only one word. It actually outputs a probability score for all the words it knows (the model’s “vocabulary”, which can range from a few thousand to over a million words). The keyboard application then has to find the words with the highest scores, and present those to the user.
<div class="imgcap">
<img src="https://user-images.githubusercontent.com/22668421/61177912-8376bc00-a5ae-11e9-9832-5f271a250ae5.png" style="border:none;width:100%">
</div>

Words get their embeddings by us looking at which other words they tend to appear next to. The mechanics of that is that

1. We get a lot of text data (say, all Wikipedia articles, for example). then
2. We have a window (say, of three words) that we slide against all of that text.
3. The sliding window generates training samples for our model

As this window slides against the text, we (virtually) generate a dataset that we use to train a model. To look exactly at how that’s done, let’s see how the sliding window processes this phrase:

Thou shalt not make a machine in the likeness of a human mind” ~Dune

When we start, the window is on the first three words of the sentence:

<div class="imgcap">
<img src="https://user-images.githubusercontent.com/22668421/61177938-8de58580-a5af-11e9-8215-d7a6e92196ea.png" style="border:none;width:100%">
</div>

We take the first two words to be features, and the third word to be a label，We then slide our window to the next position and create a second sample




## Skipgram

Instead of only looking two words before the target word, we can also look at two words after it.
<div class="imgcap">
<img src="https://user-images.githubusercontent.com/22668421/61177987-7ce94400-a5b0-11e9-826d-fef530591b3b.png" style="border:none;width:100%">
</div>

If we do this, the dataset we’re virtually building and training the model against would look like this:
<div class="imgcap">
<img src="https://user-images.githubusercontent.com/22668421/61177995-9a1e1280-a5b0-11e9-9ee0-996a1660d8f1.png" style="border:none;width:100%">
</div>

This is called a Continuous Bag of Words architecture and is described in one of the word2vec papers.
Another architecture that also tended to show great results does things a little differently.
Instead of guessing a word based on its context (the words before and after it), this other architecture tries to guess neighboring words using the current word. We can think of the window it slides against the training text as looking like this:
<div class="imgcap">
<img src="https://user-images.githubusercontent.com/22668421/61178005-dbaebd80-a5b0-11e9-9834-9fb2284b3703.png" style="border:none;width:100%">
</div>

The pink boxes are in different shades because this sliding window actually creates four separate samples in our training dataset:
<div class="imgcap">
<img src="https://user-images.githubusercontent.com/22668421/61178010-0a2c9880-a5b1-11e9-88fe-eb98efa474cf.png" style="border:none;width:100%">
</div>

This method is called the skipgram architecture. We can visualize the sliding window as doing the following
<div class="imgcap">
<img src="https://user-images.githubusercontent.com/22668421/61178013-2597a380-a5b1-11e9-9608-bd4f2d2a3a7f.png" style="border:none;width:100%">
</div>

This would add these four samples to our training dataset.

Now that we have our skipgram training dataset that we extracted from existing running text, let’s glance at how we use it to train a basic neural language model that predicts the neighboring word.

<div class="imgcap">
<img src="https://user-images.githubusercontent.com/22668421/61178019-5546ab80-a5b1-11e9-833f-78c447c4ef58.png" style="border:none;width:100%">
</div>

We start with the first sample in our dataset. We grab the feature and feed to the untrained model asking it to predict an appropriate neighboring word.
<div class="imgcap">
<img src="https://user-images.githubusercontent.com/22668421/61178027-7a3b1e80-a5b1-11e9-92ef-1b49416d93c1.png" style="border:none;width:100%">
</div>

The model conducts the three steps and outputs a prediction vector (with a probability assigned to each word in its vocabulary). Since the model is untrained, it’s prediction is sure to be wrong at this stage. But that’s okay. We know what word it should have guessed – the label/output cell in the row we’re currently using to train the model:

<div class="imgcap">
<img src="https://user-images.githubusercontent.com/22668421/61178037-b2daf800-a5b1-11e9-877a-8aae57d26d88.png" style="border:none;width:100%">
</div>

How far off was the model? We subtract the two vectors resulting in an error vector:
<div class="imgcap">
<img src="https://user-images.githubusercontent.com/22668421/61178042-d0a85d00-a5b1-11e9-8d97-49d34d4af246.png" style="border:none;width:100%">
</div>

This error vector can now be used to update the model so the next time, it’s a little more likely to guess thou when it gets not as input.

While this extends our understanding of the process, it’s still not how word2vec is actually trained. We’re missing a couple of key ideas.

## Negative Sampling

Recall the three steps of how this neural language model calculates its prediction

The third step is very expensive from a computational point of view – especially knowing that we will do it once for every training sample in our dataset (easily tens of millions of times). We need to do something to improve performance.

To generate high-quality embeddings using a high-performance model, we can switch the model’s task from predicting a neighboring word:

And switch it to a model that takes the input and output word, and outputs a score indicating if they’re neighbors or not (0 for “not neighbors”, 1 for “neighbors”).

This simple switch changes the model we need from a neural network, to a logistic regression model – thus it becomes much simpler and much faster to calculate.

This switch requires we switch the structure of our dataset – the label is now a new column with values 0 or 1. They will be all 1 since all the words we added are neighbors.


<div class="imgcap">
<img src="https://user-images.githubusercontent.com/22668421/61178070-03068a00-a5b3-11e9-9966-0baf5535a2a3.png" style="border:none;width:100%">
</div>

This can now be computed at blazing speed – processing millions of examples in minutes. But there’s one loophole we need to close. If all of our examples are positive (target: 1), we open ourself to the possibility of a smartass model that always returns 1 – achieving 100% accuracy, but learning nothing and generating garbage embeddings.

To address this, we need to introduce negative samples to our dataset – samples of words that are not neighbors. Our model needs to return 0 for those samples. Now that’s a challenge that the model has to work hard to solve – but still at blazing fast speed. We randomly sample words from our vocabulary

<div class="imgcap">
<img src="https://user-images.githubusercontent.com/22668421/61178077-482abc00-a5b3-11e9-89a1-75831d65eca2.png" style="border:none;width:100%">
</div>

This idea is inspired by Noise-contrastive estimation. We are contrasting the actual signal (positive examples of neighboring words) with noise (randomly selected words that are not neighbors). This leads to a great tradeoff of computational and statistical efficiency.


## Word2vec Training Process
This part is very important if you want to implement word2vec correctly.
Now that we’ve established the two central ideas of skipgram and negative sampling, we can proceed to look closer at the actual word2vec training process.

Before the training process starts, we pre-process the text we’re training the model against. In this step, we determine the size of our vocabulary (we’ll call this vocab_size, think of it as, say, 10,000) and which words belong to it.

At the start of the training phase, we create two matrices – an Embedding matrix and a Context matrix. These two matrices have an embedding for each word in our vocabulary (So vocab_size is one of their dimensions). The second dimension is how long we want each embedding to be (embedding_size – 300 is a common value, but we’ve looked at an example of 50 earlier in this post).

<div class="imgcap">
<img src="https://user-images.githubusercontent.com/22668421/61178108-c8e9b800-a5b3-11e9-8012-95f1bc0aebae.png" style="border:none;width:100%">
</div>

At the start of the training process, we initialize these matrices with random values. Then we start the training process. In each training step, we take one positive example and its associated negative examples. Let’s take our first group:

<div class="imgcap">
<img src="https://user-images.githubusercontent.com/22668421/61178114-ee76c180-a5b3-11e9-83cb-44656117458f.png" style="border:none;width:100%">
</div>

Now we have four words: the input word not and output/context words: thou (the actual neighbor), aaron, and taco (the negative examples). We proceed to look up their embeddings – for the input word, we look in the Embedding matrix. For the context words, we look in the Context matrix (even though both matrices have an embedding for every word in our vocabulary).

<div class="imgcap">
<img src="https://user-images.githubusercontent.com/22668421/61178122-1403cb00-a5b4-11e9-9d92-27028c1edb82.png" style="border:none;width:100%">
</div>

Then, we take the dot product of the input embedding with each of the context embeddings. In each case, that would result in a number, that number indicates the similarity of the input and context embeddings
<div class="imgcap">
<img src="https://user-images.githubusercontent.com/22668421/61178125-2c73e580-a5b4-11e9-857f-03e534c38841.png" style="border:none;width:100%">
</div>

Now we need a way to turn these scores into something that looks like probabilities – we need them to all be positive and have values between zero and one. This is a great task for sigmoid, the logistic operation.

<div class="imgcap">
<img src="https://user-images.githubusercontent.com/22668421/61178129-457c9680-a5b4-11e9-86a6-8d649f716eff.png" style="border:none;width:100%">
</div>

And we can now treat the output of the sigmoid operations as the model’s output for these examples. You can see that taco has the highest score and aaron still has the lowest score both before and after the sigmoid operations.

Now that the untrained model has made a prediction, and seeing as though we have an actual target label to compare against, let’s calculate how much error is in the model’s prediction. To do that, we just subtract the sigmoid scores from the target labels.

Here comes the “learning” part of “machine learning”. We can now use this error score to adjust the embeddings of not, thou, aaron, and taco so that the next time we make this calculation, the result would be closer to the target scores.

<div class="imgcap">
<img src="https://user-images.githubusercontent.com/22668421/61178134-6e049080-a5b4-11e9-9fd0-fc660f442095.png" style="border:none;width:100%">
</div>

This concludes the training step


## code
[Mikolov’s original implementation in C](https://github.com/tmikolov/word2vec/blob/master/word2vec.c)

Very well written, looks much nicer than my first attempt of python implementation.

