---
layout: post
comments: true
mathjax: true
priority: 440000
title: “Why sigmoid function output can be interpreted as the proabability of y=1”
tags: [Machine Learning]
img: ml.png
excerpt: Machine Learning
date: 2019-07-13 12:00:00
---
## Problem setting
For binary classification question, what we need to answer is that given a sample $$x$$, what is the proabability that the label of this sample is 1.


## Assumption
Can we view sigmoid function output as the proabability of y=1? yes ! It is true under the assumption that every given class, the conditional distribution of the variable is Guassian and the standard deviation are the same. 

## Proof

$$
\begin{aligned} P\left(C_{1} | x\right) &=\frac{P\left(C_{1}, x\right)}{P(x)} 
\\ &=\frac{P\left(x|C_{1}\right) P\left(C_{1}\right)}{p\left(x|C_{1}\right) P\left(C_{1}\right)+P\left(x|C_{2}\right) P\left(c_{1}\right)} 
\\ &=\frac{1}{1+\frac{P\left(x | C_{2}\right) P\left(C_{2}\right)}{p\left(x | C_{1}\right) P\left(C_{1}\right)}}
\\ &=\frac{1}{1+e^{-a}} \quad[\text {sigmoid}] \end{aligned}
$$

where 

$$
a=\operatorname{In} \frac{P\left(x | C_{1}\right) P\left(C_{1}\right)}{P\left(x | C_{2}\right) P\left(C_{2}\right)}
$$


when $$x$$ is a continuous variable, assume the conditional probability of $$x$$ for both two classes are from Guassian distribution and the standard deviation are equal.

$$
\begin{array}{l}{P\left(x | C_{1}\right) \sim N\left(x | \mu_{1}, \Sigma\right)=\frac{1}{(2 \pi)^{D / 2}|\Sigma|^{1 / 2}} \exp \left\{-\frac{1}{2}\left(x-\mu_{1}\right)^{T} \Sigma^{-1}\left(x-\mu_{1}\right)\right\}} \\ {P\left(x | C_{2}\right) \sim N\left(x | \mu_{2}, \Sigma\right)=\frac{1}{(2 \pi)^{D / 2}|\Sigma|^{1 / 2}} \exp \left\{-\frac{1}{2}\left(x-\mu_{2}\right)^{T} \Sigma^{-1}\left(x-\mu_{2}\right)\right\}}\end{array}
$$

then we have 

$$
\begin{aligned} \operatorname{InP}\left(x | C_{1}\right) &=-\frac{D}{2} \operatorname{In}(2 \pi)-\frac{1}{2} \operatorname{In}|\Sigma|-\frac{1}{2}\left(x-\mu_{1}\right)^{T} \Sigma^{-1}\left(x-\mu_{1}\right) \\ \operatorname{In} P\left(x | C_{2}\right) &=-\frac{D}{2} \operatorname{In}(2 \pi)-\frac{1}{2} \operatorname{In}|\Sigma|-\frac{1}{2}\left(x-\mu_{2}\right)^{T} \Sigma^{-1}\left(x-\mu_{2}\right) \end{aligned}
$$

where $$D$$ is the dimension of variable $$x$$.

so,

$$
\begin{aligned} a(x) &=\operatorname{In} P\left(x | C_{1}\right)-\operatorname{In} P\left(x | C_{2}\right)+\operatorname{In} \frac{P\left(C_{1}\right)}{P\left(C_{2}\right)} \\ &=\left(\mu_{1}-\mu_{2}\right)^{T} \Sigma^{-1} x-\frac{1}{2} \mu_{1}^{T} \Sigma^{-1} \mu_{1}+\frac{1}{2} \mu_{2}^{T} \Sigma^{-1} \mu_{2}+\operatorname{In} \frac{P\left(C_{1}\right)}{P\left(C_{2}\right)} \\ &=w^{T} x+w_{0} \end{aligned}
$$

gotcha!!!


For multiple classes senario: 
$$
\begin{aligned} P\left(C_{k} | x\right) &=\frac{P\left(x | C_{k}\right) P\left(C_{k}\right)}{\sum_{j} P\left(x | C_{j}\right) P\left(C_{j}\right)} \\ &=\frac{\exp \left(a_{k}\right)}{\sum_{j} \exp \left(a_{j}\right)} \end{aligned}
$$

This is softmax function! Bingo!