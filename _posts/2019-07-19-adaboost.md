---
layout: post
comments: true
mathjax: true
priority: 440000
title: “Adaboost”
tags: [Machine Learning]
excerpt: Machine Learning
img: ml.png
date: 2019-07-19 12:00:00
---

## $$\gamma$$-Weak-Learnability

1. A learning algorithm, A, is a $$\gamma$$-weak-learner for a class $$H$$ if there exists a function $$
m_{\mathcal{H}} :(0,1) \rightarrow \mathbb{N}$$ such that for every $$\delta \in(0,1)$$, for every distribution $$D$$ over $$X$$ , and for every labeling function $$f : \mathcal{X} \rightarrow\{ \pm 1\}$$, if the realizable assumption holds with respect to $$H$$,$$D$$,$$f$$, then when running the learning algorithm on $$m \geq m_{\mathcal{H}}(\delta) \text { i.i.d. }$$ examples generated by $$D$$ and labeled by $$f$$, the algorithm returns a hypothesis $$h$$ such that, with probability of at least $$1-\delta, L_{(\mathcal{D}, f)}(h) \leq 1 / 2-\gamma$$
2. A hypothesis class $$H$$ is $$\gamma$$-weak-learnable if there exists a $$\gamma$$-weak-learner for that class.