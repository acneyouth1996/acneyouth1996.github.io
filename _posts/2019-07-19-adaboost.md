---
layout: post
comments: true
mathjax: true
priority: 440000
title: “Adaboost”
tags: [Machine Learning]
excerpt: Machine Learning
img: ml.png
date: 2019-07-19 12:00:00
---

## $$\gamma$$-Weak-Learnability

1. A learning algorithm, A, is a $$\gamma$$-weak-learner for a class $$H$$ if there exists a function $$
m_{\mathcal{H}} :(0,1) \rightarrow \mathbb{N}$$ such that for every $$\delta \in(0,1)$$, for every distribution $$D$$ over $$X$$ , and for every labeling function $$f : \mathcal{X} \rightarrow\{ \pm 1\}$$, if the realizable assumption holds with respect to $$H$$,$$D$$,$$f$$, then when running the learning algorithm on $$m \geq m_{\mathcal{H}}(\delta) \text { i.i.d. }$$ examples generated by $$D$$ and labeled by $$f$$, the algorithm returns a hypothesis $$h$$ such that, with probability of at least $$1-\delta, L_{(\mathcal{D}, f)}(h) \leq 1 / 2-\gamma$$
2. A hypothesis class $$H$$ is $$\gamma$$-weak-learnable if there exists a $$\gamma$$-weak-learner for that class.

This definition is almost identical to the definition of PAC learning, which here we will call strong learning, with one crucial difference: Strong learnability implies the ability to find an arbitrarily good classifier (with error rate at most $$\epsilon$$ for an arbitrarily small $$\epsilon>0$$). In weak learnability, however, we only need to output a hypothesis whose error rate is at most $$1 / 2-\gamma$$, namely, whose error rate is slightly better than what a random labeling would give us. The hope is that it may be easier to come up with efficient weak learners than with efficient (full) PAC learners.

## AdaBoost

AdaBoost (short for Adaptive Boosting) is an algorithm that has access to a weak learner and finds a hypothesis with a low empirical risk. The AdaBoost algorithm receives as input a training set of examples $$S=\left(\mathbf{x}_{1}, y_{1}\right), \ldots,\left(\mathbf{x}_{m}, y_{m}\right)$$ where for each $$i, y_{i}=f\left(\mathbf{x}_{i}\right)$$ 
for some labeling function $$f$$. The boosting process proceeds in a sequence of consecutive rounds. At round $$t$$, the booster first defines a distribution over the examples in $$S$$, denoted $$\mathbf{D}^{(t)}$$. That is, $$\mathbf{D}^{(t)} \in \mathbb{R}_{+}^{m}$$ and $$\sum_{i=1}^{m} D_{i}^{(t)}=1$$. Then, the booster passes the distribution $$\mathbf{D}^{(t)}$$ and the sample $$S$$ to the weak learner. The weak learner is assumed to return a “weak” hypothesis, $$h_{t}$$, whose error,
$$
\epsilon_{t} \stackrel{\mathrm{def}}{=} L_{\mathbf{D}^{(t)}}\left(h_{t}\right) \stackrel{\mathrm{def}}{=} \sum_{i=1}^{m} D_{i}^{(t)} \mathbb{1}_{\left[h_{t}\left(\mathbf{x}_{i}\right) \neq y_{i}\right]}
$$

is at most $$
\frac{1}{2}-\gamma
$$

The pseudocode of AdaBoost is presented in the following.

<div class="imgcap">
<img src="https://user-images.githubusercontent.com/22668421/61553526-982cd700-aa28-11e9-8918-1299936c615e.png" style="border:none;width:100%">
</div>
